[DNNMark]
run_mode=composed

[Convolution]
name=conv0
n=64
c=3
h=227
w=227
previous_layer=null
conv_mode=convolution
kernel_size=7
num_output=64
pad=3
stride=2
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=conv0
name=conv0_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=conv0_bn
name=conv0_relu
activation_mode=relu

[Pooling]
previous_layer=conv0_relu
name=pool1
pool_mode=max
kernel_size=2
pad=0
stride=2

[Convolution]
previous_layer=conv0_relu
name=block1_1_conv
conv_mode=convolution
kernel_size=3
num_output=64
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block1_1_conv
name=block1_1_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block1_1_bn
name=block1_1_relu
activation_mode=relu

[Convolution]
previous_layer=block1_1_relu
name=block1_2_conv
conv_mode=convolution
kernel_size=3
num_output=64
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block1_2_conv
name=block1_2_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block1_2_bn
name=block1_2_relu
activation_mode=relu

[Convolution]
previous_layer=block1_2_relu
name=block1_3_conv
conv_mode=convolution
kernel_size=3
num_output=64
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block1_3_conv
name=block1_3_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block1_3_bn
name=block1_3_relu
activation_mode=relu

[Convolution]
previous_layer=block1_3_relu
name=block1_4_conv
conv_mode=convolution
kernel_size=3
num_output=64
pad=1
stride=2
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block1_4_conv
name=block1_4_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block1_4_bn
name=block1_4_relu
activation_mode=relu

[Convolution]
previous_layer=block1_4_relu
name=block2_1_conv
conv_mode=convolution
kernel_size=3
num_output=128
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block2_1_conv
name=block2_1_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block2_1_bn
name=block2_1_relu
activation_mode=relu

[Convolution]
previous_layer=block2_1_relu
name=block2_2_conv
conv_mode=convolution
kernel_size=3
num_output=128
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block2_2_conv
name=block2_2_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block2_2_bn
name=block2_2_relu
activation_mode=relu

[Convolution]
previous_layer=block2_2_relu
name=block2_3_conv
conv_mode=convolution
kernel_size=3
num_output=128
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block2_3_conv
name=block2_3_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block2_3_bn
name=block2_3_relu
activation_mode=relu

[Convolution]
previous_layer=block2_3_relu
name=block2_4_conv
conv_mode=convolution
kernel_size=3
num_output=128
pad=1
stride=2
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block2_4_conv
name=block2_4_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block2_4_bn
name=block2_4_relu
activation_mode=relu

[Convolution]
previous_layer=block2_4_relu
name=block3_1_conv
conv_mode=convolution
kernel_size=3
num_output=256
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block3_1_conv
name=block3_1_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block3_1_bn
name=block3_1_relu
activation_mode=relu

[Convolution]
previous_layer=block3_1_relu
name=block3_2_conv
conv_mode=convolution
kernel_size=3
num_output=256
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block3_2_conv
name=block3_2_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block3_2_bn
name=block3_2_relu
activation_mode=relu

[Convolution]
previous_layer=block3_2_relu
name=block3_3_conv
conv_mode=convolution
kernel_size=3
num_output=256
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block3_3_conv
name=block3_3_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block3_3_bn
name=block3_3_relu
activation_mode=relu

[Convolution]
previous_layer=block3_3_relu
name=block3_4_conv
conv_mode=convolution
kernel_size=3
num_output=256
pad=1
stride=2
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block3_4_conv
name=block3_4_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block3_4_bn
name=block3_4_relu
activation_mode=relu

[Convolution]
previous_layer=block3_4_relu
name=block4_1_conv
conv_mode=convolution
kernel_size=3
num_output=512
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block4_1_conv
name=block4_1_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block4_1_bn
name=block4_1_relu
activation_mode=relu

[Convolution]
previous_layer=block4_1_relu
name=block4_2_conv
conv_mode=convolution
kernel_size=3
num_output=512
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block4_2_conv
name=block4_2_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block4_2_bn
name=block4_2_relu
activation_mode=relu

[Convolution]
previous_layer=block4_2_relu
name=block4_3_conv
conv_mode=convolution
kernel_size=3
num_output=512
pad=1
stride=1
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block4_3_conv
name=block4_3_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block4_3_bn
name=block4_3_relu
activation_mode=relu

[Convolution]
previous_layer=block4_3_relu
name=block4_4_conv
conv_mode=convolution
kernel_size=3
num_output=128
pad=1
stride=2
conv_fwd_pref=fastest
conv_bwd_filter_pref=fastest
conv_bwd_data_pref=fastest

[BatchNorm]
previous_layer=block4_4_conv
name=block4_4_bn
batchnorm_mode=per_activation
save_intermediates=true
exp_avg_factor=1
epsilon=2e-5

[Activation]
previous_layer=block4_4_bn
name=block4_4_relu
activation_mode=relu

[FullyConnected]
previous_layer=block4_4_relu
name=fc
num_output=1000

[Softmax]
name=softmax
previous_layer=fc
softmax_algo=accurate
softmax_mode=channel
